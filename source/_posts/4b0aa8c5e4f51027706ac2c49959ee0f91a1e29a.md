---
title: 4b0aa8c5e4f51027706ac2c49959ee0f91a1e29
date: 2017-03-14 11:56:34
tags: treeboosting
---
# 原理介绍
  分类器是在机器学期领域中对数据进行统计、分类及预测的一类重要方法，各种不同的分类器被广泛应用于工业界及各类数据挖掘的比赛中。目前已知的分类器方法有一大类主要基
于集成学习，基于这个思想目前已知的有引导聚集算法或装袋法(Bagging)、自适应增强算法(Adaboost)、多重增强算法(Multiboost)、梯度增强算法(Gradient Boosting)和极限梯
度增强算法(XGBoost)。本文将先介绍引导聚集算法和自适应增强算法作为引入，再重点介绍本文所使用的极限梯度增强算法。
## 集成学习方法概述
对于一个分类器来说，如果其模型结构较为复杂，则其很容易适应当前数据，即对于当前数据拟合有很好的效果，误差较小，但是这样很容易造成对数据的过度拟合(Overfitting)，不利于对于未知的数据进行预测。反之，如果一个模型结构较为简单，则虽然不至于对训练数据过度拟合，但有可能在训练集中产生较大的误差，这同样不利于分类器对未知的数
据进行预测。集成学习方法综合了以上两个方向的有点来做机器学期，其目标是在模型的复杂度和数据模型拟合和准确率之间做一个最优的平衡，以达到优秀的预测效果。
对于一个给定的数据集，集成学习方法往往先将这个数据集分割成N个数据子集，在对这N个子集中的数据分别做训练调节分类器的结构和参数，最后对于一个未知数据，集成学习方
法用这个N子集所预测出的N的结果用事先约定好的方法做集成，做出最终预测。
<!-- more -->
### 自引导抽样法
这类方法是对于一个给定的数据集S, 生成N个数据子集的一类经典方法。自引导抽样法是一个有放回的抽样法，在假定每个元素都处于连续均匀分布情况下对每个元素进行等概率抽样，抽完样后将元素放回。例如，对于一个给定的数据集$D=\\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\\}$，其中$x_i\in{R^{d}}, y_i\in{R}, |D|=m$， 数据集中有m个元素，在经过k次、每次抽取n的元素的抽样后，对于第i个数据集。对于某个数据子集，由于它是由有放回抽样的方法获取的，所以数据子集中的元素可能重复，但是数据子集中的每一个元素都属于原来给定的数据集$D$。
在这个有m的元素的数据集D中，每个元素每次被抽中的概率为$1\over m$, 那么每个元素不被抽中的概率就是$1-{1\over m}$, 所以在经过足够多次的抽样后，每个元素仍不被
选进用于训练的数据子集的概率为$\lim{m \to \infty} {(1-{1\over m})}^{m} = {1\over e} \approx 0.368$。换言之，利用自引导抽样法可以将约$63.2\%$的原始数据作为训练集，而将其余的作为测试集。
自引导抽样法是生成训练数据的一种方法，本文其余部分生成数据子集的方法包括但不限于此方法。
### 引导聚集算法


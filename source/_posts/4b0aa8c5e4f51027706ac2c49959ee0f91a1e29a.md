---
title: 4b0aa8c5e4f51027706ac2c49959ee0f91a1e29
date: 2017-03-14 11:56:34
tags: treeboosting
---
# 原理介绍
  分类器是在机器学期领域中对数据进行统计、分类及预测的一类重要方法，各种不同的分类器被广泛应用于工业界及各类数据挖掘的比赛中。目前已知的分类器方法有一大类主要基于集成学习，基于这个思想目前已知的有引导聚集算法或装袋法(Bagging)、自适应增强算法(Adaboost)、多重增强算法(Multiboost)、梯度增强算法(Gradient Boosting)和极限梯度增强算法(XGBoost)。本文将先介绍引导聚集算法和自适应增强算法作为引入，再重点介绍本文所使用的极限梯度增强算法。
## 集成学习方法概述
对于一个分类器来说，如果其模型结构较为复杂，则其很容易适应当前数据，即对于当前数据拟合有很好的效果，误差较小，但是这样很容易造成对数据的过度拟合(Overfitting)，不利于对于未知的数据进行预测。反之，如果一个模型结构较为简单，则虽然不至于对训练数据过度拟合，但有可能在训练集中产生较大的误差，这同样不利于分类器对未知的数据进行预测。集成学习方法综合了以上两个方向的有点来做机器学习，其目标是在模型的复杂度和数据模型拟合和准确率之间做一个最优的平衡，以达到优秀的预测效果。
对于一个给定的数据集，集成学习方法往往先将这个数据集分割成N个数据子集，在对这N个子集中的数据分别做训练调节分类器的结构和参数，形成一个弱分类器，最后对于一个未知数据，集成学习方法用这个N子集所预测出的N的结果用事先约定好的方法做集成，做出最终预测。
<!-- more -->
### 自引导抽样法
这类方法是对于一个给定的数据集S, 生成N个数据子集的一类经典方法。自引导抽样法是一个有放回的抽样法，在假定每个元素都处于连续均匀分布情况下对每个元素进行等概率抽样，抽完样后将元素放回。例如，对于一个给定的数据集$D=\\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\\}$，其中$x_i\in{R^{d}}, y_i\in{R}, |D|=m$， 数据集中有m个元素，在经过k次、每次抽取n的元素的抽样后，对于第i个数据集。对于某个数据子集，由于它是由有放回抽样的方法获取的，所以数据子集中的元素可能重复，但是数据子集中的每一个元素都属于原来给定的数据集$D$。
在这个有m的元素的数据集D中，每个元素每次被抽中的概率为$1\over m$, 那么每个元素不被抽中的概率就是$1-{1\over m}$, 所以在经过足够多次的抽样后，每个元素仍不被选进用于训练的数据子集的概率为$\lim{m\to \infty}{(1-{1\over m})}^{m} = {1\over e} \approx 0.368$。换言之，利用自引导抽样法可以将约$63.2\%$的原始数据作为训练集，而将其余的作为测试集。
自引导抽样法是生成训练数据的一种方法，本文其余部分生成数据子集的方法包括但不限于此方法。
### 引导聚集算法
引导聚集算法是一个相对基本的增强学习方法，其基本原理是先利用自引导抽样法生成N个数据子集，再对每个数据子集分别训练分类器，当有需要预测的数据时，算法会让事先训练好的N个数据子集对应的N个分类器分别做预测，最后通过类似投票的方式产生最终结果。当对于某个特定的数据子集进行训练时，可以采用类似于L2范式的标准设计目标函数，测量训练目标与设计目标的欧几里得距离，目标函数的值越小说明训练效果越好。例如，对于第$i$个数据子集而言，其目标函数可以设计成$\sum_{i}^{n}{1\over 2}{(y_i-\hat{y_i})}^2$。对于预测结果，可以让所有的弱分类器对结果进行等权重投票，得票最高的数据即为最终结果。
### 自适应增强算法
自适应增强算法是基于引导聚集算法的改进，它的工作原理是对于不同的元素根据分类或预测错误率设定抽样是的权重。例如在对第i个数据子集的预测完成后，算法会对第i次的预测进行评估，并筛选出预测结果有有误的部分，根据误差情况在将样本放回时适当地增加这些被错误预测的样本的抽样概率，这样在对第i+1次数据子集进行训练时，在第i轮被错误预测的数据会被重点训练，以此增强训练效果。例如，在第i轮训练时，第2、3、5号样本被错误预测，则本轮训练结束后在将样本放回时会增加2、3、5号样本被抽中的概率。在第i+1轮训练时，第2、3、5号样本就有很大的概率成为第i+1轮数据子集中的元素，这样第i+1轮训练出类来的分类器会适当地提高对第2、3、5号样本预测的精确度，而在第i+1轮训练结束后被错误预测的样本又会有很大的概率出现在下一轮的数据子集中。
## 极限增强算法
这个算法是由Tianqi Chen在2015年提出，是由梯度增强算法演变而来，目前已在Kaggle等数据挖掘类比赛中获得广泛应用并取得良好效果。
### 核心算法
#### 算法背景
对于一个数据集来说，我们可以用线性的模型来做回归以对未知的数据进行预测,即利用${\hat{y^i}}=\sum^{j}w^jx^{ij}$来做回归。其中${\hat{y_i}}$为预测结果,${w^j}$为参数，${x^{ij}}$为第i个数据样本的第j的维度。而对于一个线性模型来说，其最终的学习目的为学习一系列参数，即$\Theta=\\{w_j|j=1,2,...d\\}$。当有了最终的学习目标以后，我们希望让参数集合的表现足够优秀，于是我们可以设定目标函数以衡量参数的拟合程度。
一般地，为了同时达到数据准确拟合和避免过度拟合两个目标，我们会选用两个不同的函数来共同衡量拟合的准确度，即$Obj(\Theta)=L(\Theta)+\Omega(\Theta)$。其中，$L(\Theta)$为损失函数(Training loss)，这是一个用于衡量数据拟合准确度的函数；$\Omega(\Theta)$为正规化函数，这是一个用于衡量数据模型复杂度的指标，可以指示数据模型是否过度拟合。对于$L(\Theta)$来说，有不同的方式可以做测量，例如二次损失函数和逻辑损失函数(4)。对于$\Omega(\Theta)$来说，其衡量标准可选用L1范数和L2范数等(4、5)。上述连个函数是为了分别达到不同的子目标而设计的。损失函数的设计目标是让参数与训练数据尽量拟合，其值越小，说明其拟合度越高，与数据的真实分布就越接近。但是，由于模型较高的复杂度可能导致过度拟合，所以损失函数的值在足够小的时候可能反而不如未过度拟合的参数更接近真实分布，于是我们有了第二个设计目标，即正规化函数。正规化函数的值越高，则说明参数模型的复杂度越低，参数的选择就越不容易过度拟合，然而，如果正则化函数的值足够低，这可能也不利于参数的选择，因为过低的模型复杂度会导致损失函数的值过高。所以我们选择将这两个函数的值叠加来作为最终的目标函数。
#### 回归树
回归时，又称分类与回归树，是由普通决策树演变而来的一种分类器，其树形结构与决策逻辑与决策树类似，但与之不同的是回归树在每个叶子节点都包含一个值，回归树最终预测的结果同样采用了集成学习的思想，即利用不同的弱回归树的预测结果的和来作为最终预测结果。例如，如图(9)中的一课回归树，其树形结构与叶子节点的值都已经给定，节点的值代表某人与“爱玩电子游戏”的相关度，那么如果输入的数据是“男孩”，那么最终的预测值为左侧与右侧两棵树树和预测结果加和得到，即2+0.9=2.9。同理，如果输入的数据是“老爷爷”，那么最终的预测结果就是-1.9。回归树是一类优秀的在机器学期中的监督学习方法，目前有近半数的数据挖掘比赛中是由基于树的集成学习相关方法夺得冠军。回归树的优点在于，与输入数据的尺度无关，即不需要做正则化，另外可以支持高维度、大规模的数据。对于一个回归树来说，一般设计树的结构和参数的方法都是通过一些启发式的原则来决定，例如熵减，剪枝，高度限制和平滑化等，而这些启发式的规则恰好对应于目标函数中可量化的设计，即熵减对应损失函数，简直对应节点正规化，高度限制对应对函数空间的限制，平滑化对应在叶子节点处对其数值做做L2正规化。
#### 梯度增强
对于一个给定的目标函数(20.1)，我们可以采用叠加训练法选取参数，从一个常量预测结果开始，分多轮进行训练，每轮训练都会计算上一轮训练的结果与训练数据的残差，每轮训练时新的回归树都以最大限度弥补残差为目标。即从${\hat{y}_{i0}=0}$开始，训练
（20.2345)。在做叠加训练时，在第t轮迭代中，其预测结果为(21.1)，而在第t轮中目标函数可以写成(21.2)，如果采用二次损失函数作为目标函数额衡量标准，那么目标函数可以最终写成(21.3)，其中(21.3.1)为残差。所以现在目标转化为了对(22.1)的优化。这个函数仍然不易做直接优化，但是可以利用泰勒展开的原理将(22.1)近似为(22.4)，有了这个新的目标函数，我们就可以利用$f_t(x_i)$的一阶导数和二阶导数来的信息来做优化。这个新的目标函数除了对损失函数的设计，还有对树结构的复杂度的定义，即$\Omega(f_t)$。
为了更好地说明树的概念，我们对一棵叶子几点有权重的树做如下定义(24)。对于一棵树的复杂度，其定义为(25)。例如，对于(25)中的归回树，$T=3, w_1=+2, w_2=+0.1, w_3=-1$， 这棵树的复杂度计算结果为(25.2)。
在同时有了对损失函数和正规化函数的定义以后，我们可以以(26.4)作为最终目标。
